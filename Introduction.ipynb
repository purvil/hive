{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SQL like query tool to query data stored in HDFS and other filesystem that integrate with Hadoop.\n",
    "* Deveoped by facebook.\n",
    "* It processes structrued data that can be stored into tables\n",
    "* Hive query is converted into MR program and that will be run on HDFS.\n",
    "* Hive is NOT a database. It points to data in HDFS\n",
    "* Hive is NOT tool for OLTP. Does not provide row level insert, update, delete.\n",
    "* Not used when fast response time is needed. Used where high latency is acceptable with batch processing.\n",
    "* Hive is built on write once and read many concept. SQL is built on write many and read many concept.\n",
    "* Hive is good for batch processing, SQL for transaction processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "#### Hive CLI\n",
    "* UI, user directly interacts via this. Hive CLI, web interface and thrift server (JDBC/ODBC connection).\n",
    "* Driver, Convert SQL query to MR program with help of compiler. Receives query from UI.\n",
    "* Compiler: Convert Hive SQL query to MR program. Also create execution plan with help of metastore.\n",
    "* Metastore is small db which has all structural info like, table, partition, number of columns and its data type, serializer, deseializer.\n",
    "    - It is not stored on HDFS, as it requires to have random reads and update, HDFS is good for sequential read.\n",
    "* Execution engine is connected with hadoop framework. Executes plan created by compile. Contacts name node and resource manager to fetch desired data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have to create database to store metadata of our table. It will NOT store actual data, actual data is in HDFS.\n",
    "* It is called metastore.\n",
    "\n",
    "```\n",
    "create database d1;\n",
    "\n",
    "or\n",
    "\n",
    "create database if not exists d1;\n",
    "\n",
    "or \n",
    "\n",
    "create database if not exists d1 comment \"This is a database\";\n",
    "\n",
    "or\n",
    "\n",
    "create database if not exists d1 with dbproperties('creator'='purvil','date'='2019-07-15')\n",
    "```\n",
    "\n",
    "```\n",
    "create table if not exists table1 (col1 string, col2 array<string>, col3 string, col4 int) row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\\n' stored as textfile;\n",
    "```\n",
    "\n",
    "```\n",
    "describe database d1;\n",
    "\n",
    "describe database extended d1;\n",
    "```\n",
    "\n",
    "```\n",
    "show databases; // show all created databases\n",
    "```\n",
    "\n",
    "```\n",
    "use d1; //now onwards all table data will be stored in d1 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table with location\n",
    "* By default hive store data in `hive.metastore.warehouse.dir` variable.\n",
    "```\n",
    "set hive.metastore.warehouse.dir=/user/hive/warehouse;\n",
    "```\n",
    "* To store in other location provide path explicitly\n",
    "```\n",
    "create table if not exists table1 (col1 string, col2 array<string>, col3 string, col4 int) row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\\n' stored as textfile location '/user/purvil/dir_name';\n",
    "```\n",
    "\n",
    "#### Link metadata to file\n",
    "```\n",
    "load data local inpath '/home/purvil/dir_name/data.txt' into table table1; // for data in local FS\n",
    "load data inpath '/home/purvil/dir_name/data.txt' into table table1; // for data in local HDFS\n",
    "```\n",
    "* Here `into` means append the data. We can also use `overwrite` which "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal vs External table\n",
    "* For internal table hive is owner of table data and metadata. For external table, hive is only responsible for table's metadata. So if we drop internal table, all our data and metadata is gone. After dropping external, hive will not have linkage to data stored in HDFS.\n",
    "\n",
    "```\n",
    "create table if not exists table7 (col1 string, col2 string, col3 int) row format delimited fields terminated by ',', collection items terminated by ':' lines terminated by '\\n' stored as textfile;\n",
    "```\n",
    "* Above is syntax to create internal table.\n",
    "* We can drop internal table as\n",
    "```\n",
    "drop table table7\n",
    "```\n",
    "* To create external table\n",
    "```\n",
    "create external table if not exists table7 (col1 string, col2 string, col3 int) row format delimited fields terminated by ',', collection items terminated by ':' lines terminated by '\\n' stored as textfile;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `set hive.cli.print.header=true;` Which prints column name of table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tables can have more or less columns than target file or table. But make sure to match column type, otherwise it will be filled with null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert\n",
    "#### Loading data into table from another table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "create table tab (col1 int, col2 string, col3 string) stored as textfile; // here we do not have to define delimiter as it is already specified for table from where we are loading data.\n",
    "\n",
    "insert into table tab select id,fname,lname from table1; // append data to table tab\n",
    "insert overwrite table tab select id,fname,lname from table1 where lname = 'dave'; // overwrite data to table tab\n",
    "```\n",
    "\n",
    "#### Multi Insert\n",
    "```\n",
    "from emp_tab insert into table developer_tab select col1,col2,col3 where col3 = 'Developer' insert into table manager_tab select col1, col2, col3 where col3='Mgr';\n",
    "```\n",
    "\n",
    "### Alter table\n",
    "```\n",
    "alter table tab add columns(col4 string, col5 int);\n",
    "```\n",
    "* It will be null as long as new file is not loaded.\n",
    "\n",
    "```\n",
    "alter table tab change col1 col1 int after col3; // move col1 after col3\n",
    "alter table tab change column col2 new_col2 string; // rename col\n",
    "alter table tab rename to tab1; // rename table\n",
    "alter table tab1 replace columns (id int, name string); // replace all column names\n",
    "```\n",
    "\n",
    "### Table properties \n",
    "```\n",
    "desc formatted tab1;  // list all metadata of table\n",
    "alter table tab1 set tblproperties('prop'='val')\n",
    "alter table tab1 set fileformat avro;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORDER BY\n",
    "* Full sorting of data, so uses single reducer.\n",
    "```\n",
    "select col2 from table5 order by col2 limit 5;\n",
    "```\n",
    "\n",
    "### SORT BY\n",
    "* sorting within reducer\n",
    "```\n",
    "select col2 from table5 sort by col2\n",
    "```\n",
    "\n",
    "### DISTRIBUTE BY\n",
    "* Used to distribute key-value pairs among the reducer. Decided which key will go to which reducer. Guarantees that each reducer will get non overlapping values. It does not gurantees sorting.\n",
    "```\n",
    "select col2 from table5 distribute by col2\n",
    "```\n",
    "\n",
    "### CLUSTER BY\n",
    "* Combination of distribute by sort by command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/map_reduce.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function\n",
    "\n",
    "#### unix_timestamp\n",
    "* Return number of seconds between specified time and epoch (1970-01-01 00:00:00)\n",
    "```\n",
    "select unix_timestamp('2017-04-26 00:00:00')\n",
    "```\n",
    "\n",
    "#### from_unixtime\n",
    "* Give date from seconds\n",
    "```\n",
    "select from_unixtime(123456);\n",
    "```\n",
    "\n",
    "#### TO_DATE(string timestamp) \n",
    "* Returns date type of given string\n",
    "\n",
    "#### YEAR(string date)\n",
    "* Return YEAR part of the date\n",
    "\n",
    "#### MONTH(string date)\n",
    "* Return MONTH part of the date\n",
    "\n",
    "#### HOUR, DAY, MINUTE, SECOND, WEEKOFYEAR, DATEDIFF(string date1, string date2), DATE_ADD(string date, int days), DATE_SUB(string date, int days)\n",
    "\n",
    "#### ciel(9.5)\n",
    "* It will return 10\n",
    "\n",
    "#### floor(10.9)\n",
    "* return 10\n",
    "\n",
    "#### round(123.456, 2)\n",
    "* return 123.46\n",
    "\n",
    "#### rand()\n",
    "* Generate random number\n",
    "\n",
    "#### concat\n",
    "```\n",
    "select concat(col1, '-', col3) from table1;\n",
    "```\n",
    "\n",
    "#### length, lower, upper, lpad(col_nam2, 10, 'v'), rpad(col_nam2, 10, 'v'), ltrim(str), rtrim(str), repeat(col_name, 2), reverse(col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split\n",
    "```\n",
    "select split('hive:hadoop', ':');\n",
    "```\n",
    "* Returns hive and hadoop\n",
    "\n",
    "#### substring\n",
    "```\n",
    "select substr('hive is querying tool', 4); // starting from index 4, first index is 1.\n",
    "select substr('hive is querying tool', 4, 3); // starting from index 4 total 3 characters, first index is 1. \n",
    "```\n",
    "\n",
    "#### instr\n",
    "```\n",
    "select instr('hive is querying tool', 'e'); // Returns index of first occurence of e.\n",
    "```\n",
    "\n",
    "#### Condition\n",
    "\n",
    "```\n",
    "select if(col3='England', col1, col4) from table2; // if condition is met choose col1 else col4\n",
    "select isnull(col1) from table1;\n",
    "select isnotnull(col1) from table1;\n",
    "select coalesce(col1,col2,col3) from table1; // select first non null value\n",
    "select NVL(col1,col2) from table1; // if col1 is null then col2 is selected else col1.\n",
    "```\n",
    "\n",
    "#### Explode\n",
    "* Takes array as input and outputs the elements of array as separate rows. \n",
    "```\n",
    "select explode(col_name) as mycol from table1;\n",
    "```\n",
    "* Using it we can only select exploded column\n",
    "\n",
    "#### lateral view\n",
    "* Overcome limitation of explode, we can select any number of columns.\n",
    "* Create virtual table of exploded column and join it with original table.\n",
    "```\n",
    "select col1, col_name from table2 lateral view explode(col3) dummy as col_name; // virtual view is stored in virtual table called dummy with col_name column\n",
    "```\n",
    "* It is used to take out key:value as column from map data type.\n",
    "```\n",
    "select key,value from table3 lateral view explode(map_col) dummy as key,value;\n",
    "```\n",
    "\n",
    "#### RLIKE\n",
    "* Right like\n",
    "* Any substring of A matches with B then returns true.\n",
    "```\n",
    "select 'hadoop' rlike 'ha'; // returns true\n",
    "select 'hadoop' rlike 'ha*'; // returns true\n",
    "select null rlike 'ha'; // returns null\n",
    "select 'hadoop' rlike null; // returns null\n",
    "```\n",
    "\n",
    "#### RANK\n",
    "* Gives ranking within ordered partition. Ties are assigned same rank, with next ranking skipped.\n",
    "\n",
    "#### DENSE_RANK\n",
    "* Gives ranking within ordered partition, rank are consecutive. No rank are skipped if there are rank with multiple items\n",
    "\n",
    "#### ROW_NUMBER\n",
    "* Assign unique number to each row within the partition given by order by clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/rank.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning and Bucketing\n",
    "* Ways to organize table data in parts.\n",
    "* Ex. Partition data on department basis. So if we condition on department, we do not have to search entire table.\n",
    "\n",
    "#### Static Partitioning\n",
    "```\n",
    "create table table_name (deptno int, empname string, sal int) partitioned by (deptname string) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile;\n",
    "\n",
    "// insert data from another table\n",
    "insert into table table_name partition(deptname='part_name') select col1, col3, col4 from dept where col2='Accounts';\n",
    "\n",
    "// insert data from file\n",
    "load data local inpath 'path/to/file' into table table_name partition(deptname='part_name')\n",
    "```\n",
    "\n",
    "#### Dynamic Partitioning\n",
    "* `set hive.exec.dynamic.partition=true;`\n",
    "* `set hive.exec.dynamic.partition.mode = nonstrict;`\n",
    "```\n",
    "create table table_name (deptno int, empname string, sal int) partitioned by (deptname string) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile;\n",
    "\n",
    "insert into table table_name partition(deptname) select col1, col3, col4, col2 from dept; // column on which partition is done should be at last.\n",
    "```\n",
    "\n",
    "```\n",
    "show partitions table_name;\n",
    "alter table table_name drop partition (deptname='HR');\n",
    "alter table table_name add partition (deptname='DEV');\n",
    "```\n",
    "\n",
    "* Another way to create partition is create directory directly, but doing so will not update metadata.\n",
    "* So, we have to run `msck repair table table_name;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In bucketing, all the same column values of a bucketed column will go to same bucket. Can be used along with partition.\n",
    "* Partition data by department, then bucket data within partition by bucketing over salary.\n",
    "* `set hive.enforce.bucketing = true;`\n",
    "* `set hive.exec.dynamic.partition.mode = nonstrict;`\n",
    "```\n",
    "create table table_name (deptno int, empname string, sal int, location string) partition by(deptname string) clustered by (location) into 4 buckets row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile\n",
    "\n",
    "insert into table_name partition(deptname) select col1, col3, col4, col5, col2 from dept;\n",
    "```\n",
    "\n",
    "* number of buckets = number of reduce task.\n",
    "* Bucket is physically a file..\n",
    "* We can set number of buckets during creation of table. Bucketed Map joins are fastest joins. Both joining table should be bucketed on same column as of joining column and both tables should have equal number of buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Sampling\n",
    "* Sample is small portion of data.\n",
    "* Limit can grab data from only 1 partition. To make representative sample we have to use table sampling.\n",
    "```\n",
    "select * from table_name tablesample (bucket 1 out of 2 on location);\n",
    "```\n",
    "* It divide total 4 buckets in group of 2 buckets and select 1 of that. To make smaller selection use 1 out of 3.\n",
    "\n",
    "```\n",
    "select * from table_name tablesample (2 percent); // Select 2 % of dataset.\n",
    "select * from table_name tablesample (1M); // select 1 MB of data\n",
    "select * from table_name tablesample (20 rows);\n",
    "```\n",
    "\n",
    "### no_drop\n",
    "* Prevent dropping of table or partition\n",
    "```\n",
    "alter table table_name enable no_drop;\n",
    "alter table table_name disable no_drop;\n",
    "alter table table_name partition ('deptname'='HR') enable no_drop;\n",
    "```\n",
    "\n",
    "### offline\n",
    "* Prevents data from being queried\n",
    "```\n",
    "alter table table_name enable offline;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join\n",
    "#### Inner join\n",
    "```\n",
    "select * from table1 join table2 on (table1.id = table2.id)\n",
    "```\n",
    "\n",
    "#### Outer join\n",
    "* Left, Right, Full\n",
    "```\n",
    "select * from table1 left outer join table2 on (table1.id = table2.id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default last table in join is streamed and other are buffered in memory. We can explicitly specify which table should be stremed\n",
    "```\n",
    "select /*+ STREAMTABLE (table_name) */ ....\n",
    "```\n",
    "* How to do join on mapper side? (map join)\n",
    "    - Mention in query itself `select /*+ MAPJOIN(table_name) */ ....`\n",
    "    - `set hive.auto.convert.join=true` `set hive.mapjoin.smalltable.filesize=25000000`\n",
    "* Full outer join can NOT be performed via this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIEWS\n",
    "* Virtual table created as a result of a hive query on a table.\n",
    "* Does not contains any data on its own.\n",
    "* All DML operations can be performed on views\n",
    "* Can be created by selecting any number of columns of its base table.\n",
    "* Views are read only.\n",
    "* Once created, the schema of view is frozen and is independent of changes made to base table schema.\n",
    "* Drop the table and we can not fire query on views.\n",
    "```\n",
    "create view view_name as select * from table_name;\n",
    "```\n",
    "* It can be used to hide underlying table from some users.\n",
    "* Protect base table from dropped or altered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "* Acts as reference to record\n",
    "* Used to speed up searching of data.\n",
    "* Will search for only portion of data and not whole dataset.\n",
    "* Partition done at hdfs, indexing done at table level.\n",
    "```\n",
    "create index index_name on table table_name(col_name) as 'COMPACT' with deferred rebuild;\n",
    "```\n",
    "* COMPACT and BITMAP. \n",
    "    - COMAPCT : Stores pair of index column value and block id\n",
    "    - BITMAP : Stores pair of index column value and rows.\n",
    "* When data gets changed we will have to rebuild index.\n",
    "```\n",
    "alter index index_name on table table_name(col_name);\n",
    "```\n",
    "```\n",
    "show formatted index on table9;\n",
    "```\n",
    "* We can create multiple index on the same table.\n",
    "* Use index when\n",
    "    - Dataset is large\n",
    "    - Speed is concerned\n",
    "    - Frequent use of where clause in queirs\n",
    "* Do not use\n",
    "    - When dataset is unique\n",
    "    - No frequent use of where clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF (User defined function)\n",
    "* Create Java program\n",
    "* Save and convert the program to jar file\n",
    "* Add jar file to Hive.\n",
    "    - `add jar /path/to/jar;`\n",
    "* Create function of the Jar file added.\n",
    "    - `create temporary function f2 as 'com.hive1.ud2';`\n",
    "* Use those functions in Hive query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Properties\n",
    "* Passive properties just tag a table (name, date of creation)\n",
    "* Active properties, helps in data processing.\n",
    "```\n",
    "create table table_name (col1 string, col2 int) row format delimited fields terminated ',' lines terminated by '\\n' stored as textfile tblproperties(\"skip.header.line.count\"=\"3\");\n",
    "```\n",
    "* another property for footer is `\"skip.footer.line.count\"=\"3\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Immutable table\n",
    "```\n",
    "create table table3(col1 string, col2 int) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile tblproperties(\"immutable\"=\"true\")\n",
    "```\n",
    "* It allows to load data only 1 time. After that we can not change table3. Insert overwrite will work always.\n",
    "* Drop vs truncate:\n",
    "    - Drop remove metadata and data for a table for internal table. For external table linkage between HDFS and hive is removed, metadata is removed.\n",
    "    - Truncate only remove data from table. Metadata or structure will not be removed.\n",
    "```\n",
    "drop table table_name;\n",
    "truncate table table_name;\n",
    "```\n",
    "* Purge is table property. If it is set to true, during dropping internal table, data will NOT go to trash it will be gone permanently.\n",
    "```\n",
    "create table table_name (col1 string, col2 int) row format delimited fields terminated by ',' lines terminated by '\\n' stored as textfile tblproperties(\"auto.purge\"=\"true\");\n",
    "```\n",
    "* Null format property\n",
    "    - By default nothing is null between delimiter.\n",
    "    - To declare null use `tblproperties(\"serialization.null.format\"=\"\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACID / Transactional properties\n",
    "* Only ORC file format is supported for transactional feature.\n",
    "* Table must be bucketed\n",
    "* Begin, commit, rollback features are also not supported\n",
    "* Reading/writing to a acid table is only allowed in a session where transactional properties are true. All operation are auto commit.\n",
    "```\n",
    "create table table_name (id int, emp_name string, dep string) clustered by (id) into 4 buckets stored as orc tblproperties(\"transactional\"=\"true\");\n",
    "```\n",
    "* Update of bucketing column is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC file format properties\n",
    "```\n",
    "stored as orc tblproperties(\"orc.format\"=\"zlib\")\n",
    "show tblproperties table_name;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To run hive command from terminal\n",
    "```\n",
    "hive -e 'select * from table_name;'\n",
    "```\n",
    "\n",
    "* To run hive script from hive shell\n",
    "    - save hive script with .hql extension\n",
    "```\n",
    "source /path/to/file.hql;\n",
    "```\n",
    "* To run hive script from terminal\n",
    "    - `hive -f /path/to/script.hql`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables in Hive\n",
    "* `set deptno=40;`\n",
    "* Check value `set deptno;`\n",
    "* `set hiveconf:d1=20;`\n",
    "```\n",
    "select * from table_name where col6=${hiveconf:deptno};\n",
    "```\n",
    "* hivevar variable:\n",
    "    - sets value globally, hiveconf sets value locally.\n",
    "```\n",
    "set hivevar:deptnumber=10;\n",
    "```\n",
    "* hiveconf is local, hivevar is global."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting variable in terminal\n",
    "```\n",
    "hive --hiveconf var_name=20 -e 'select * from tablename where col6=${hiveconf:var_name}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression\n",
    "* Small size, occupy less space, less network bandwidth.\n",
    "* Compressing input file\n",
    "* Compressing map output\n",
    "* Compressing reducer output.\n",
    "\n",
    "### Hiverc\n",
    "* Executed when we launch the hive session.\n",
    "* Setting column header to make visible\n",
    "* Making current database name part of the hive prompt\n",
    "* Adding any jars or files\n",
    "* Registering UDFs.\n",
    "* Put it in hive conf directory.\n",
    "\n",
    "### Cartesian product\n",
    "```\n",
    "select * from table2, table3;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archiving\n",
    "* Packing files to less used storage\n",
    "* Old data is archived mostly\n",
    "* Stored as HAR file\n",
    "* Decreases load on name node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEZ\n",
    "* Distributed execution framework to run data processing application on hadoop.\n",
    "* Built on hadoop yarn to execute DAG of general data processing task.\n",
    "* Advanced successor of MapReduce.\n",
    "* Tez is better than MR.\n",
    "* Tez represents data processing as dataflow graph.\n",
    "* Vertices as application logic and edges as movement of data.\n",
    "* Enables Yarn framework to allocate resource intelligently.\n",
    "* Multiple reduce task\n",
    "    - MR will create one MR job per reduce task, more reads and write of data.\n",
    "    - TEZ link all reduce task, data can be passed directly to one part of reducer to other.\n",
    "* In memory vs disk write\n",
    "    - In MR data is shuffled across nodes regardless of data size.\n",
    "    - Tez allows small datasets to be handled fully in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML data\n",
    "* We have to install appropriate serde. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count\n",
    "* my file is like word, word, word, word.\n",
    "```\n",
    "create table word_count(line string) stored as textfile;\n",
    "load data local inpath '/path/to/file' into table word_count;\n",
    "select word, count(1) as count from (select explode(split(line, ',')) as word from word_count) w group by word; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use power of 2 number of bucket for optimized join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Advantage of columnar storage like RCfile, ORC (optimized row columnar), Parquet.\n",
    "     - Different compression per different type of column.\n",
    "     - skip whole column when not needed.\n",
    "     - ORC file has index info, like max and min value in column stripe, so we can skip entire that strip. Also contains count and sum info."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
